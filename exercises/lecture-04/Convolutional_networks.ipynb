{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JI_Sad9fd_6J"
   },
   "source": [
    "# Convolutional Neural Networsk Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g4f91NyeFcf"
   },
   "source": [
    "## Sub-project 1:\n",
    "\n",
    "## Solve Fashion_MNIST with LeNet architecture\n",
    "\n",
    "In this project you will implement the LeNet architecture of Convolutional Neural Networks. First you will download the Fashion-MNIST dataset. Split into train/validation/test datasets and train the network. Finally, plot the learning curves (train/validation loss and accuracy) and show the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # to handle matrix and data operation\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOfUQvRVfvmA"
   },
   "source": [
    "### 1. Download Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "50Qusl-ud7BK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(2),           # pads 28x28 to 32x32\n",
    "    transforms.ToTensor(),       # converts to PyTorch tensor\n",
    "])\n",
    "\n",
    "data = datasets.FashionMNIST(\n",
    "    root = 'data/mnist',\n",
    "    train = True,\n",
    "    transform = transform,\n",
    "    download = True,\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data/mnist',\n",
    "    train = False,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfvQ12mVf02d"
   },
   "source": [
    "### 2. Split the data into train / validation / test subsets. Make mini-batches, if necesssary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mY1rOmYOf7Ir"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 50000\n",
      "Validation set size: 10000\n",
      "Mini batch training set size: 500\n",
      "Mini batch validation set size: 100\n",
      "Mini batch test set size: 100\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(data, [50000, 10000])\n",
    "\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Validation set size: {len(val_set)}\")\n",
    "\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_set,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1),\n",
    "\n",
    "    'validation'  : torch.utils.data.DataLoader(val_set,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1),\n",
    "    'test'  : torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1),\n",
    "}\n",
    "print(f\"Mini batch training set size: {len(loaders['train'])}\")\n",
    "print(f\"Mini batch validation set size: {len(loaders['validation'])}\")\n",
    "print(f\"Mini batch test set size: {len(loaders['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZBdYurpf7zr"
   },
   "source": [
    "### 3. Build the LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ilN859L7kq04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 16, 5, 5])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5),          # input: (1, 32, 32), output: (6, 28, 28)\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, 2)           # output: (6, 14, 14)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 5),         # input: (6, 14, 14), output: (16, 10, 10)\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, 2)           # output: (16, 5, 5)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 120, 5),        # input: (16, 5, 5), output: (120, 1, 1)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "conv_nn = CNN()\n",
    "print(conv_nn)\n",
    "\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(conv_nn.parameters(), lr=0.001)\n",
    "for parameter in conv_nn.parameters():\n",
    "    print(parameter.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hos3qlAkrCH"
   },
   "source": [
    "### 4. Train the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffBB5ZUPlETU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.2574996976852417\n",
      "Validation Loss: 0.31616960480809214\n",
      "-------------------\n",
      "Validation Loss Decreased(inf--->31.61696048080921) \t Saving The Model\n",
      "Epoch 2\n",
      "Training Loss: 0.24570678855478764\n",
      "Validation Loss: 0.30274931967258456\n",
      "-------------------\n",
      "Validation Loss Decreased(31.61696048080921--->30.274931967258453) \t Saving The Model\n",
      "Epoch 3\n",
      "Training Loss: 0.23961457602679728\n",
      "Validation Loss: 0.2943728061020374\n",
      "-------------------\n",
      "Validation Loss Decreased(30.274931967258453--->29.437280610203743) \t Saving The Model\n",
      "Epoch 4\n",
      "Training Loss: 0.22976759906113148\n",
      "Validation Loss: 0.30019139364361763\n",
      "-------------------\n",
      "Epoch 5\n",
      "Training Loss: 0.22094300243258477\n",
      "Validation Loss: 0.296290482878685\n",
      "-------------------\n",
      "Epoch 6\n",
      "Training Loss: 0.21411211340129374\n",
      "Validation Loss: 0.305352770537138\n",
      "-------------------\n",
      "Epoch 7\n",
      "Training Loss: 0.20750077272951603\n",
      "Validation Loss: 0.29769605785608294\n",
      "-------------------\n",
      "Epoch 8\n",
      "Training Loss: 0.19912269833683968\n",
      "Validation Loss: 0.3042208893597126\n",
      "-------------------\n",
      "Epoch 9\n",
      "Training Loss: 0.19342266257107257\n",
      "Validation Loss: 0.3094433160871267\n",
      "-------------------\n",
      "Epoch 10\n",
      "Training Loss: 0.18729324421286583\n",
      "Validation Loss: 0.3134524843096733\n",
      "-------------------\n",
      "Epoch 11\n",
      "Training Loss: 0.18020456785708666\n",
      "Validation Loss: 0.30341842144727704\n",
      "-------------------\n",
      "Epoch 12\n",
      "Training Loss: 0.17564218160510064\n",
      "Validation Loss: 0.29993977963924406\n",
      "-------------------\n",
      "Epoch 13\n",
      "Training Loss: 0.16897946218401194\n",
      "Validation Loss: 0.31451552361249924\n",
      "-------------------\n",
      "Epoch 14\n",
      "Training Loss: 0.16526959739625455\n",
      "Validation Loss: 0.3189642562717199\n",
      "-------------------\n",
      "Epoch 15\n",
      "Training Loss: 0.1589154503196478\n",
      "Validation Loss: 0.31597974747419355\n",
      "-------------------\n",
      "Epoch 16\n",
      "Training Loss: 0.15477468280494214\n",
      "Validation Loss: 0.31912999793887137\n",
      "-------------------\n",
      "Epoch 17\n",
      "Training Loss: 0.14785182313621045\n",
      "Validation Loss: 0.3246915332973003\n",
      "-------------------\n",
      "Epoch 18\n",
      "Training Loss: 0.1426361404657364\n",
      "Validation Loss: 0.32403924986720084\n",
      "-------------------\n",
      "Epoch 19\n",
      "Training Loss: 0.13856853613257408\n",
      "Validation Loss: 0.3349034260213375\n",
      "-------------------\n",
      "Epoch 20\n",
      "Training Loss: 0.13412867546454071\n",
      "Validation Loss: 0.3334345106780529\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "loss_train = []\n",
    "loss_valid = []\n",
    "\n",
    "def train(num_epochs, nn, loaders):\n",
    "  min_valid_loss = np.inf\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    nn.cuda()\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    # Train the model\n",
    "    epoch_train_loss = 0\n",
    "    # This line tells our NN that it's in the training mode\n",
    "    # This will become relevant when we introduce layers that behave\n",
    "    # differently in training and deployment/evaluation modes\n",
    "    nn.train()\n",
    "    for i, (images, labels) in enumerate(loaders['train']):\n",
    "      if torch.cuda.is_available():\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "      output = nn(images)\n",
    "      loss = loss_f(output, labels)\n",
    "      epoch_train_loss += loss.item()\n",
    "\n",
    "      # clear gradients for this training step\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # backpropagation, compute gradients\n",
    "      loss.backward()\n",
    "      # apply gradients\n",
    "      optimizer.step()\n",
    "\n",
    "    # Validate the model\n",
    "    epoch_val_loss = 0\n",
    "    nn.eval()\n",
    "    for images_v, labels_v in loaders['validation']:\n",
    "      if torch.cuda.is_available():\n",
    "        images_v, labels_v = images_v.cuda(), labels_v.cuda()\n",
    "      output = nn(images_v)\n",
    "      loss_v = loss_f(output, labels_v)\n",
    "      epoch_val_loss += loss_v.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'Training Loss: {epoch_train_loss / len(loaders[\"train\"])}')\n",
    "    print(f'Validation Loss: {epoch_val_loss / len(loaders[\"validation\"])}')\n",
    "    print('-------------------')\n",
    "    if min_valid_loss > epoch_val_loss:\n",
    "      print(f'Validation Loss Decreased({min_valid_loss}--->{epoch_val_loss}) \\t Saving The Model')\n",
    "      min_valid_loss = epoch_val_loss\n",
    "      # Saving State Dict\n",
    "      torch.save(nn.state_dict(), 'saved_model.pth')\n",
    "\n",
    "train(20, conv_nn, loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD273At6lEgQ"
   },
   "source": [
    "### 5. Plot the training curves (Loss and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcGvcDT7lJI5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vgNFn0clJXj"
   },
   "source": [
    "### 6. Show the confusion matrix and accuracy on the test dataset. (Don't do confussion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHo4TZwSlM5d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_lkzGiUlNGb"
   },
   "source": [
    "### 7. Is LeNet better than the fully connected Neural Network trained in the project from lecture 3? Comment on the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oP9i42GllssB"
   },
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoEkp5vBlvMd"
   },
   "source": [
    "## Sub-project 2:\n",
    "\n",
    "## Use a pretrained model to solve\n",
    "\n",
    "In this project you will download the Intel Image Classification dataset (https://www.kaggle.com/puneet6060/intel-image-classification/download).\n",
    "Find a suitable pretrained Convolutional Neural Network and its weights. Fix the filters and retrain/finetune the top of the network. Show and comment on resuls.\n",
    "\n",
    "Alternativelly, you can browse Kaggle for some interesting datasets like: Dogs vs Cats, Alien vs Predator, Doom vs Animal Crossing, CelebA, etc. Just don't use something easy like MNIST, Fashion-MNIST, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpP46GT6uEjW"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uASXDNLVpIHi"
   },
   "source": [
    "### 1. Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQS6d9DWpL7u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EznZoYdpMTa"
   },
   "source": [
    "### 2. Preprocess the data.\n",
    "\n",
    "(This might include resizing, augmenting, etc.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SUlBllHpnHe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dj79LHvXpmdI"
   },
   "source": [
    "### 3. Split the data (train / test / validation) and make mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPmX35GHqEfb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHNAiFSjqGS9"
   },
   "source": [
    "### 4. Download the pretrained architecture and weights.\n",
    "\n",
    "This part might include some research and understanding of architecures and the type of data they were trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPlve5ZTqPDa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da8sXeYBqPS-"
   },
   "source": [
    "### 5. Build the larger model that includes the pre-trained part and prepare it for training.\n",
    "\n",
    "* Show the model summary so you are sure which components are included and how many (un)trainable parameters you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mm2h48w2qX2f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8roaTOSOqYMy"
   },
   "source": [
    "### 6. Train the trainable part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jy3xFNKCqv7Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA5Gp6g2qwGL"
   },
   "source": [
    "### 7. Show the training plots and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0Qg82iQq1Vb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QBCyt67q1r1"
   },
   "source": [
    "### Finetune the model and show the plots, test data confusion matrix and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VrwGiObrzBJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9rnUtAVrzTn"
   },
   "source": [
    "### 9. Comment on the results. Are the results to your satisfaction? Which phase contributed to the improved performance? What was the most challenging aspect of the exercise? Ideas for improving the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZWr_zbnsE_K"
   },
   "source": [
    "Answer:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
